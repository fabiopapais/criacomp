{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/filipecalegario/criacomp/blob/main/2024_1_CRIACOMP_Experimentando_com_GRADIO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_XJcxqx22uj"
      },
      "source": [
        "# Usando Gradio para criar interfaces simples\n",
        "\n",
        "Confira a biblioteca no [GitHub](https://github.com/gradio-app/gradio-UI) e veja a página de [primeiros passos](https://gradio.app/getting_started.html) para mais demonstrações."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definindo uma função"
      ],
      "metadata": {
        "id": "2kryxMLAk2Xm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtlFLbke2Sob"
      },
      "outputs": [],
      "source": [
        "def greet(name):\n",
        "  return \"Hello \" + name + \"!\"\n",
        "\n",
        "greet(\"World\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R06dbZZaYJDq"
      },
      "source": [
        "### Instalando o gradio\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJUJLWQ92g6R"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criando uma interface simples para a função"
      ],
      "metadata": {
        "id": "Juis_wjCkrtp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e200MmBU2aLT"
      },
      "outputs": [],
      "source": [
        "import gradio\n",
        "\n",
        "gradio.Interface(greet, \"text\", \"text\").launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interface com múltiplos parâmetros de entrada"
      ],
      "metadata": {
        "id": "eFLW8qhXknh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def greet(name, intensity):\n",
        "    return \"Hello \" * intensity + name + \"!\"\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=greet,\n",
        "    inputs=[\"text\", \"slider\"],\n",
        "    outputs=[\"text\"],\n",
        ")\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "za8I9PHJj5E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parâmetros avançados de interface"
      ],
      "metadata": {
        "id": "XCTyms5zkhYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def yes_man(message, history):\n",
        "    if message.endswith(\"?\"):\n",
        "        return \"Yes\"\n",
        "    else:\n",
        "        return \"Ask me anything!\"\n",
        "\n",
        "gr.ChatInterface(\n",
        "    yes_man,\n",
        "    chatbot=gr.Chatbot(height=300),\n",
        "    textbox=gr.Textbox(placeholder=\"Ask me a yes or no question\", container=False, scale=7),\n",
        "    title=\"Yes Man\",\n",
        "    description=\"Ask Yes Man any question\",\n",
        "    theme=\"soft\",\n",
        "    examples=[\"Hello\", \"Am I cool?\", \"Are tomatoes vegetables?\"],\n",
        "    cache_examples=True,\n",
        "    retry_btn=None,\n",
        "    undo_btn=\"Delete Previous\",\n",
        "    clear_btn=\"Clear\",\n",
        ").launch()"
      ],
      "metadata": {
        "id": "M7qQkXoADrqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interfaces de Chat"
      ],
      "metadata": {
        "id": "c3QUYtyBnen7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain openai langchain_community gradio"
      ],
      "metadata": {
        "id": "GjxCM0bfez-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usando a biblioteca da OpenAI em Python"
      ],
      "metadata": {
        "id": "mzCx9YOHkBkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get('OPENAI_KEY')\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "def predict(message, history):\n",
        "    history_openai_format = []\n",
        "    for human, assistant in history:\n",
        "        history_openai_format.append({\"role\": \"user\", \"content\": human })\n",
        "        history_openai_format.append({\"role\": \"assistant\", \"content\":assistant})\n",
        "    history_openai_format.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    response = client.chat.completions.create(model='gpt-4o',\n",
        "      messages= history_openai_format,\n",
        "      temperature=1.0,\n",
        "      stream=True)\n",
        "\n",
        "    partial_message = \"\"\n",
        "    for chunk in response:\n",
        "        if chunk.choices[0].delta.content is not None:\n",
        "              partial_message = partial_message + chunk.choices[0].delta.content\n",
        "              yield partial_message\n",
        "\n",
        "gr.ChatInterface(predict).launch()"
      ],
      "metadata": {
        "id": "_W8ckTKHHOPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usando LangChain com OpenAI"
      ],
      "metadata": {
        "id": "bFAW98oPkOkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "import openai\n",
        "import gradio as gr\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_KEY')\n",
        "\n",
        "llm = ChatOpenAI(temperature=1.0, model='gpt-4o')\n",
        "\n",
        "def predict(message, history):\n",
        "    history_langchain_format = []\n",
        "    for human, ai in history:\n",
        "        history_langchain_format.append(HumanMessage(content=human))\n",
        "        history_langchain_format.append(AIMessage(content=ai))\n",
        "    history_langchain_format.append(HumanMessage(content=message))\n",
        "    gpt_response = llm(history_langchain_format)\n",
        "    return gpt_response.content\n",
        "\n",
        "gr.ChatInterface(predict).launch()"
      ],
      "metadata": {
        "id": "GTxrNZF4EJJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usando LangChain com OpenAI com streaming"
      ],
      "metadata": {
        "id": "KqQyyot8kUQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "import openai\n",
        "import gradio as gr\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_KEY')\n",
        "\n",
        "llm = ChatOpenAI(temperature=1.0, model='gpt-4o', streaming=True)\n",
        "\n",
        "def predict(message, history):\n",
        "    history_langchain_format = []\n",
        "    for human, ai in history:\n",
        "        history_langchain_format.append(HumanMessage(content=human))\n",
        "        history_langchain_format.append(AIMessage(content=ai))\n",
        "    history_langchain_format.append(HumanMessage(content=message))\n",
        "\n",
        "    # Initialize the response generator\n",
        "    response_generator = llm.stream(history_langchain_format)\n",
        "\n",
        "    # Stream responses\n",
        "    response_content = \"\"\n",
        "    for response in response_generator:\n",
        "        response_content += response.content\n",
        "        yield response_content\n",
        "\n",
        "gr.ChatInterface(predict).launch()\n"
      ],
      "metadata": {
        "id": "GpfDjRarjWqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usando LangChain agora com Gemini"
      ],
      "metadata": {
        "id": "5C1GGXcGnpPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_google_genai"
      ],
      "metadata": {
        "id": "Wwr8LtmLn5Bc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "import gradio as gr\n",
        "import os\n",
        "\n",
        "# Set the environment variable for Gemini API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('gemini_api')\n",
        "\n",
        "# Initialize the Gemini LLM with streaming support\n",
        "llm = ChatGoogleGenerativeAI(temperature=1.0, model='gemini-pro', streaming=True)  # Adjust the model name as needed\n",
        "\n",
        "def predict(message, history):\n",
        "    history_langchain_format = []\n",
        "    for human, ai in history:\n",
        "        history_langchain_format.append(HumanMessage(content=human))\n",
        "        history_langchain_format.append(AIMessage(content=ai))\n",
        "    history_langchain_format.append(HumanMessage(content=message))\n",
        "\n",
        "    # Initialize the response generator for streaming\n",
        "    response_generator = llm.stream(history_langchain_format)\n",
        "\n",
        "    # Stream responses\n",
        "    response_content = \"\"\n",
        "    for response in response_generator:\n",
        "        response_content += response.content\n",
        "        yield response_content\n",
        "\n",
        "# Launch the Gradio chat interface with the predict function\n",
        "gr.ChatInterface(predict).launch()\n"
      ],
      "metadata": {
        "id": "WFLei1STnrkr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mzCx9YOHkBkP",
        "bFAW98oPkOkx",
        "KqQyyot8kUQU"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}